{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting bare soil reflectance data from known bare soil locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import os\n",
    "\n",
    "ee.Initialize()\n",
    "ee.Initialize(project='ee-bd167')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud masking function\n",
    "def filterS2_level2A(image):\n",
    "    SCL = image.select('SCL')\n",
    "    mask01 = ee.Image(0).where(SCL.lt(8).And(SCL.gt(3)), 1)  # Put a 1 on good pixels\n",
    "    return image.updateMask(mask01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = ee.FeatureCollection('projects/ee-bd167/assets/Sampling_Fields')\n",
    "\n",
    "s2_ROI = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\") \\\n",
    "    .filterDate('2020-01-01', '2024-12-31') \\\n",
    "    .filterBounds(roi) \\\n",
    "    .sort('system:time_start', True) \\\n",
    "    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 20) \\\n",
    "    .map(filterS2_level2A);\n",
    "\n",
    "# clip to roi\n",
    "s2_ROI_clipped = s2_ROI.map(lambda image: image.clip(roi))\n",
    "\n",
    "# select B1-B12 bands\n",
    "bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
    "s2_ROI_clipped = s2_ROI_clipped.select(bands)\n",
    "\n",
    "# Function to convert DN to reflectance for a single image\n",
    "def convert_dn_to_reflectance(image):\n",
    "    # Define the scaling factors and offsets for Sentinel-2 bands (replace with actual values)\n",
    "    scaling_factors = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "    offsets = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "    # Convert DN to reflectance for each band\n",
    "    def dn_to_reflectance(band, scale, offset):\n",
    "        return band.multiply(scale).add(offset)\n",
    "\n",
    "    # Apply the conversion to the selected bands\n",
    "    reflectance_image = image.multiply(scaling_factors).add(offsets)\n",
    "\n",
    "    # Copy the properties from the original image to the new image\n",
    "    reflectance_image = reflectance_image.copyProperties(image, image.propertyNames())\n",
    "\n",
    "    return reflectance_image\n",
    "\n",
    "s2_ROI_ref = s2_ROI_clipped.map(convert_dn_to_reflectance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_ROI_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spectral indices to the image collection\n",
    "\n",
    "def add_indices(image):\n",
    "    # Add NDVI\n",
    "    image = image.addBands(image.normalizedDifference(['B8', 'B4']).rename('NDVI'))\n",
    "    # Add NBR2\n",
    "    image = image.addBands(image.normalizedDifference(['B12', 'B11']).rename('NBR2'))\n",
    "    return image\n",
    "\n",
    "s2_SI = s2_ROI_ref.map(add_indices)\n",
    "\n",
    "# remove bands B1-B12\n",
    "SIs = ['NDVI', 'NBR2']\n",
    "s2_SI = s2_SI.select(SIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the mean spectral value for each field in the collection\n",
    "def get_field_spectral_profile(image, field):\n",
    "    # Reduce the image by mean over the field\n",
    "    mean_dict = image.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=field.geometry(),\n",
    "        scale=10,  # Sentinel-2 resolution\n",
    "        bestEffort=True\n",
    "    )\n",
    "\n",
    "    # Retrieve NDVI and NBR2 values from the dictionary, default to None if missing\n",
    "    ndvi_value = mean_dict.get('NDVI', None)\n",
    "    nbr2_value = mean_dict.get('NBR2', None)\n",
    "\n",
    "    # Check if both NDVI and NBR2 have valid values (i.e., not None)\n",
    "    valid_check = ee.Number(ee.Algorithms.If(ndvi_value, 1, 0)).multiply(ee.Number(ee.Algorithms.If(nbr2_value, 1, 0)))\n",
    "\n",
    "    # Only proceed if we have valid data\n",
    "    def set_field_properties():\n",
    "        # Get the field's unique Name property (e.g., 'Buckie_3', 'Doncaster_1')\n",
    "        field_name = field.get('Name')  # Update this to match the 'Name' column in your table\n",
    "\n",
    "        # Extract the image date\n",
    "        image_date = image.date().format('YYYY-MM-dd')\n",
    "\n",
    "        # Create a dictionary with field name and spectral indices\n",
    "        field_profile = mean_dict.set('image_date', image_date).set('field_name', field_name)\n",
    "        return ee.Feature(None, field_profile)\n",
    "\n",
    "    # Return feature with properties only if valid data exists, else return None\n",
    "    return ee.Algorithms.If(valid_check, set_field_properties(), None)\n",
    "\n",
    "# Function to process all images for a specific field and return a DataFrame\n",
    "def process_field(field):\n",
    "    # Apply the function to each image in the collection for this specific field\n",
    "    SI_profiles = s2_SI.map(lambda image: get_field_spectral_profile(image, field), dropNulls=True)\n",
    "\n",
    "    # Convert the results to a list of dictionaries for further use\n",
    "    SI_profiles_list = SI_profiles.getInfo()\n",
    "\n",
    "    # Convert the list to a pandas DataFrame with properties as columns\n",
    "    if len(SI_profiles_list['features']) > 0:\n",
    "        SI_profiles = pd.DataFrame([profile['properties'] for profile in SI_profiles_list['features']])\n",
    "\n",
    "        # Convert 'image_date' to datetime format\n",
    "        SI_profiles['image_date'] = pd.to_datetime(SI_profiles['image_date'], errors='coerce')\n",
    "\n",
    "        # Separate NDVI and NBR2 into different rows\n",
    "        SI_profiles_ndvi = SI_profiles[['field_name', 'NDVI', 'image_date']].copy()\n",
    "        SI_profiles_ndvi['metric'] = 'NDVI'\n",
    "\n",
    "        SI_profiles_nbr2 = SI_profiles[['field_name', 'NBR2', 'image_date']].copy()\n",
    "        SI_profiles_nbr2['metric'] = 'NBR2'\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        SI_profiles_ndvi = SI_profiles_ndvi.rename(columns={'NDVI': 'value'})\n",
    "        SI_profiles_nbr2 = SI_profiles_nbr2.rename(columns={'NBR2': 'value'})\n",
    "\n",
    "        # Combine NDVI and NBR2 into one DataFrame\n",
    "        SI_profiles_combined = pd.concat([SI_profiles_ndvi, SI_profiles_nbr2])\n",
    "\n",
    "        # Create a new row identifier 'fieldname_metric'\n",
    "        SI_profiles_combined['fieldname_metric'] = SI_profiles_combined['field_name'] + '_' + SI_profiles_combined['metric']\n",
    "\n",
    "        # Pivot the DataFrame to get timeseries with dates as columns\n",
    "        pivoted_df = SI_profiles_combined.pivot_table(\n",
    "            index='fieldname_metric',  # Each row will be 'fieldname_NDVI' or 'fieldname_NBR2'\n",
    "            columns='image_date',      # Dates as columns\n",
    "            values='value',            # NDVI or NBR2 values\n",
    "            aggfunc='mean'             # In case there are duplicates\n",
    "        )\n",
    "\n",
    "        return pivoted_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no valid data is available\n",
    "\n",
    "# Process each field in the FeatureCollection and store DataFrames in a dictionary\n",
    "field_dataframes = {}\n",
    "fields = roi.getInfo()['features']\n",
    "\n",
    "for field in fields:\n",
    "    field_name = field['properties']['Name']  # Use 'Name' from the structure you provided\n",
    "    field_dataframes[field_name] = process_field(ee.Feature(field))\n",
    "# At this point, 'field_dataframes' contains a DataFrame for each field, identified by its 'Name'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Savitzky-Golay smoothing to each row (field-metric pair)\n",
    "def smooth_timeseries_sg(df, window_size, poly_order):\n",
    "    \"\"\"\n",
    "    Apply Savitzky-Golay smoothing to all rows (field-metric pair) in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - df: DataFrame containing the time series data (rows are field-metric pairs, columns are dates).\n",
    "    - window_size: The size of the window (must be odd).\n",
    "    - poly_order: The order of the polynomial to fit.\n",
    "    \n",
    "    Returns:\n",
    "    - smoothed_df: DataFrame with the smoothed time series.\n",
    "    \"\"\"\n",
    "    smoothed_df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original\n",
    "    for idx, row in df.iterrows():  # Iterate through each row (field-metric pair)\n",
    "        y = row.values  # Extract the values (time series)\n",
    "        \n",
    "        # Print original values for debugging\n",
    "        print(f\"Original {idx} values: {y}\")\n",
    "        \n",
    "        # Check if the length of the series is greater than or equal to the window size\n",
    "        if len(y) >= window_size and np.all(np.isfinite(y)):  # Ensure no NaN values in the data\n",
    "            yhat = savgol_filter(y, window_size, poly_order)\n",
    "            smoothed_df.loc[idx] = yhat  # Store the smoothed values\n",
    "            \n",
    "            # Print smoothed values for verification\n",
    "            print(f\"Smoothed {idx} values: {yhat}\")\n",
    "        else:\n",
    "            print(f\"Skipping smoothing for {idx} because length is less than the window size or contains NaN\")\n",
    "            smoothed_df.loc[idx] = y  # If not enough data or contains NaN, leave it unsmoothed\n",
    "    return smoothed_df\n",
    "\n",
    "# Apply the smoothing function to all DataFrames in field_dataframes\n",
    "smoothed_field_dataframes = {}\n",
    "\n",
    "for field_name, df in field_dataframes.items():\n",
    "    # Apply the Savitzky-Golay smoothing function to each DataFrame\n",
    "    print(f\"\\nSmoothing field: {field_name}\")\n",
    "    smoothed_df = smooth_timeseries_sg(df, window_size=9, poly_order=2)\n",
    "    smoothed_field_dataframes[field_name] = smoothed_df  # Store in a new dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot NDVI and NBR2 time series for a single field using dual y-axes\n",
    "def plot_dual_axis_time_series(field_name, df):\n",
    "    \"\"\"\n",
    "    Plot NDVI and NBR2 time series for a single field with dual y-axes.\n",
    "\n",
    "    Args:\n",
    "    - field_name: The name of the field (str).\n",
    "    - df: DataFrame containing the time series data for NDVI and NBR2, with dates as columns.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Extract dates, NDVI, and NBR2 data\n",
    "    dates = pd.to_datetime(df.columns)  # x-axis: Dates\n",
    "    ndvi = df.loc[field_name + '_NDVI'].values  # y-axis 1: NDVI\n",
    "    nbr2 = df.loc[field_name + '_NBR2'].values  # y-axis 2: NBR2\n",
    "\n",
    "    # Plot NDVI on the first y-axis (ax1)\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('NDVI', color='tab:green')\n",
    "    ax1.plot(dates, ndvi, color='tab:green', marker='.', label='NDVI')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "    # Create a second y-axis (ax2) that shares the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('NBR2', color='tab:orange')\n",
    "    ax2.plot(dates, nbr2, color='tab:orange', marker='.', label='NBR2')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Set the title\n",
    "    plt.title(f'{field_name}')\n",
    "\n",
    "    # Use MaxNLocator to set more x-ticks\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(20))\n",
    "\n",
    "    # # Add minor ticks to x-axis\n",
    "    # ax1.xaxis.set_minor_locator(plt.MultipleLocator(20))\n",
    "\n",
    "    # Format the x-axis to show dates cleanly\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "for field_name, df in smoothed_field_dataframes.items():\n",
    "    plot_dual_axis_time_series(field_name, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the date with lowest NDVI before the peaks\n",
    "for field_name, df in smoothed_field_dataframes.items():\n",
    "    ndvi_values = df.loc[field_name + '_NDVI'].values\n",
    "    ndvi_dates = pd.to_datetime(df.columns)\n",
    "    ndvi_peaks = np.where(ndvi_values == np.max(ndvi_values))[0]\n",
    "    ndvi_peak_date = ndvi_dates[ndvi_peaks[0]]\n",
    "    print(f\"Field: {field_name}, Peak NDVI: {np.max(ndvi_values)}, Peak Date: {ndvi_peak_date}\")\n",
    "\n",
    "    # Find the date with the lowest NDVI before the peak\n",
    "    ndvi_values_before_peak = ndvi_values[:ndvi_peaks[0]]\n",
    "    ndvi_dates_before_peak = ndvi_dates[:ndvi_peaks[0]]\n",
    "    lowest_ndvi_before_peak = np.min(ndvi_values_before_peak)\n",
    "    lowest_ndvi_date_before_peak = ndvi_dates_before_peak[np.where(ndvi_values_before_peak == lowest_ndvi_before_peak)[0][0]]\n",
    "    print(f\"Lowest NDVI before peak: {lowest_ndvi_before_peak}, Date: {lowest_ndvi_date_before_peak}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the CSV files\n",
    "output_dir = \"C:\\\\Users\\\\bd167\\\\OneDrive - University of Leicester\\\\Documents\\\\Data\\\\Jupyter\\\\Sampling\\\\Reflectance\\\\\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "file_paths = {}\n",
    "for field_name, df in smoothed_field_dataframes.items():\n",
    "    file_path = os.path.join(output_dir, f\"{field_name}_smoothed.csv\")\n",
    "    df.to_csv(file_path)\n",
    "    file_paths[field_name] = file_path\n",
    "\n",
    "# Provide the file paths of the saved CSV files to the user\n",
    "file_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
